{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Charles Mendelson Pinata Farm Analytics Project\n",
    "\n",
    "### Design decisions:\n",
    "#### 1. Rather than load all of the .csv files into one data frame to manipulate, I kept them separate for 2 reasons, the first was I ran into memory issues when I tried combining them. The second was if something failed I would have staged backups that would speed up development.\n",
    "#### 2. I chose to run sentiment analysis only on English language tweets. While I could have analyzed other languages I would have no ability to QA the findings\n",
    "#### 3. I have never done sentiment analysis before, and the method I chose was one that scored on multiple emotions rather than positive negative axis.\n",
    "#### 4. The emotional lexicon came from http://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm\n",
    "#### 5. To speed up processing, I removed neutral words from the lexicon\n",
    "#### 6. The goal is to prepare the data for analysis in tableau\n",
    "#### 7. I have never done sentiment analysis before, and this solution felt a little brute forcey, but I did think of another of optimizations that could speed it up.\n",
    "#### 8. If I had more time, I'd refactor the code to be more concise, but it does work.\n",
    "#### 9. The sentiment analysis is crude. It produces counts of words with more sentiment, but it doesn't handle sarcasm, negatives, or other emotional nuance well."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Modules Needed"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "from nltk import word_tokenize\n",
    "from sqlalchemy import create_engine\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import re\n",
    "from datetime import datetime\n",
    "from nltk.corpus import stopwords\n",
    "import psycopg2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "functions needed to clean data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#creates mask for loc\n",
    "def mask(df_w_column, value):\n",
    "    return df_w_column == value\n",
    "\n",
    "#function to filter based on mask, currently only handles one filter, will be extended to handle more\n",
    "def mask_filter(df, masks, conditions=1, invert=False, condition_type = None):\n",
    "    if conditions == 1 & invert == False:\n",
    "        return df.loc[masks].reset_index(drop=True)\n",
    "    if conditions == 1 & invert == True:\n",
    "       return df.loc[~masks].reset_index(drop=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "globs tweet .csv data and cleans each csv for sentiment analysis"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "path = r'take_home_assignment/tweets'\n",
    "test_path = r'take_home_assignment/test_path'\n",
    "def batch_csvs(path):\n",
    "\n",
    "    all_files = glob.glob(path + \"/*.csv\")\n",
    "\n",
    "\n",
    "    destination = r'take_home_assignment/cleaned_tweets/'\n",
    "    # an NLTK list of words that are fill words\n",
    "    stopword_pattern = re.compile(r'\\b(' + r'|'.join(stopwords.words('english')) + r')\\b\\s*')\n",
    "    # The other regexes are to handle common data cleanup problems\n",
    "    url_pattern = re.compile(pattern=r'http\\S+', flags=re.MULTILINE)\n",
    "    at_pattern = re.compile(pattern=r'@\\S+', flags=re.MULTILINE)\n",
    "    hash_pattern = re.compile(pattern=r'#.*?(?=\\s|$)', flags=re.MULTILINE)\n",
    "    special_char_pattern = re.compile(pattern=r'[^a-zA-Z0-9 -]', flags=re.MULTILINE)\n",
    "    counter = 0\n",
    "    for filename in all_files:\n",
    "        print(filename)\n",
    "        df = pd.read_csv(filename, index_col=None, header=0)\n",
    "        mask_lang = mask(df_w_column=df['lang'], value='en')\n",
    "        df = df.loc[mask_lang].reset_index(drop=True)\n",
    "        # Ideally I would make these more concise, possibly with a list comprehension using pd.apply\n",
    "        df['cleaned_text'] = df['text'].str.lower()\n",
    "        df['cleaned_text'] = df['cleaned_text'].replace(to_replace=url_pattern, value='', regex=True)\n",
    "        df['cleaned_text'] = df['cleaned_text'].replace(to_replace=at_pattern, value='', regex=True)\n",
    "        df['hashtags'] = df['cleaned_text'].str.findall(r'#.*?(?=\\s|$)')\n",
    "        df['cleaned_text'] = df['cleaned_text'].replace(to_replace=hash_pattern, value='', regex=True)\n",
    "        df['cleaned_text'] = df['cleaned_text'].replace(to_replace=special_char_pattern, value='', regex=True)\n",
    "        # removes\n",
    "        df['cleaned_text'] =  df['cleaned_text'].replace(to_replace=stopword_pattern, value='', regex=True)\n",
    "        df['length'] = df['cleaned_text'].str.len()\n",
    "        # Removes rows where the tweet had no characters left after the regex operations\n",
    "        length_mask = df['length'] > 0\n",
    "        df = df.loc[length_mask]\n",
    "        df.to_csv(f'{destination}{counter}_english_lang_cleaned_tweets.csv', sep='\\t'\n",
    "                  , encoding='utf8')\n",
    "        counter +=1\n",
    "\n",
    "\n",
    "\n",
    "batch_csvs(path=test_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "operation to score tweets (this got a little messy because I needed to move it out of the function so it would be cached in memory"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# cleans up the corpus\n",
    "\n",
    "lexicon_file_path = 'NRC-Emotion-Lexicon/' \\\n",
    "                    'NRC-Emotion-Lexicon-v0.92/' \\\n",
    "                    'NRC-Emotion-Lexicon-Wordlevel-v0.92.txt'\n",
    "\n",
    "#grabs the lexicon\n",
    "df_emolex = pd.read_csv(lexicon_file_path,\n",
    "                        names=[\"word\", \"emotion\", \"association\"],\n",
    "                        sep='\\t')\n",
    "#pivots the lexicon so it is in a usable format\n",
    "df_emolex_words_outer = df_emolex.pivot(index='word',\n",
    "                               columns='emotion',\n",
    "                               values='association').reset_index()\n",
    "\n",
    "#sums the emotional value of each word in the lexicon\n",
    "df_emolex_words_outer['total'] = df_emolex_words_outer[['anger', 'anticipation', 'disgust'\n",
    ", 'fear', 'joy', 'negative', 'positive', 'sadness', 'surprise', 'trust']].sum(axis=1)\n",
    "\n",
    "# creates the dataframe used in the function to score the words\n",
    "emotions = df_emolex_words_outer.columns.drop('word')\n",
    "# creates a mask to remove neutral words from the lexicon\n",
    "mask_neutral_words = mask(df_emolex_words_outer['total'], 0)\n",
    "# filters the lexicon using the above mask\n",
    "df_emolex_words_outer = mask_filter(df_emolex_words_outer, mask_neutral_words, invert=True)\n",
    "# gets the first char from each word\n",
    "df_emolex_words_outer['first_char'] = df_emolex_words_outer['word'].str[0]\n",
    "df_emolex_words_outer.columns = df_emolex_words_outer.columns.get_level_values('emotion')\n",
    "# sets the word to be the index to use an iloc function instead of a loc function\n",
    "df_emolex_words_outer =  df_emolex_words_outer.set_index(['word'])\n",
    "emotions\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is the meat of the program, and it got very messy through a series of refactors to optimize\n",
    "I was able to improve the performance of the algorithm by about 50%, but it's probably putting lipstick on a pig.\n",
    "I have several thoughts on how to wring more performance out of this but I don't have any more time"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def emotional_scoring(df, column, lexicon, emotions_list):\n",
    "    new_df = df.copy()\n",
    "    df_emolex_words = lexicon\n",
    "    emotions = emotions_list\n",
    "\n",
    "    # lexicon_file_path = 'NRC-Emotion-Lexicon/' \\\n",
    "    #                 'NRC-Emotion-Lexicon-v0.92/' \\\n",
    "    #                 'NRC-Emotion-Lexicon-Wordlevel-v0.92.txt'\n",
    "    #\n",
    "    # df_emolex = pd.read_csv(lexicon_file_path,\n",
    "    #                         names=[\"word\", \"emotion\", \"association\"],\n",
    "    #                         sep='\\t')\n",
    "    #\n",
    "    # df_emolex_words = df_emolex.pivot(index='word',\n",
    "    #                                columns='emotion',\n",
    "    #                                values='association').reset_index()\n",
    "    #\n",
    "    # df_emolex_words['total'] = df_emolex_words[['anger', 'anticipation', 'disgust'\n",
    "    # , 'fear', 'joy', 'negative', 'positive', 'sadness', 'surprise', 'trust']].sum(axis=1)\n",
    "    #\n",
    "    # mask_neutral_words = mask(df_emolex_words['total'], 0)\n",
    "    # df_emolex_words = mask_filter(df_emolex_words, mask_neutral_words, invert=True)\n",
    "    # df_emolex_words['first_char'] = df_emolex_words['word'].str[0]\n",
    "    # df_emolex_words = df_emolex_words.iloc[:,0:-2]\n",
    "\n",
    "\n",
    "    # emotions = df_emolex_words.columns.drop('word')\n",
    "\n",
    "    emo_df = pd.DataFrame(0, index=df.index, columns=emotions)\n",
    "\n",
    "\n",
    "    #to stem words in the text to better match them\n",
    "    stemmer = SnowballStemmer('english')\n",
    "\n",
    "    start = datetime.now()\n",
    "    for i, row in new_df.iterrows():\n",
    "        # just to track progress as it goes to help me time and benchmark performance\n",
    "        if i % 10000 == 0:\n",
    "            tenthousand = datetime.now()\n",
    "            delta = tenthousand - start\n",
    "            print(f'it took {delta} to process {i} rows')\n",
    "        else:\n",
    "            document = word_tokenize(new_df.loc[i][column])\n",
    "            for word in document:\n",
    "                word = stemmer.stem(word.lower())\n",
    "                if len(word)< 2:\n",
    "                    pass\n",
    "                elif len(word) == 2 and word != 'ay':\n",
    "                    pass\n",
    "                else:\n",
    "                # subsets the dataframe to only loop through matching first characters\n",
    "                # I could speed it up further with a second character and do a second filter operation\n",
    "                    first_char = str(word[0])\n",
    "                # # print(f'{word} : {first_char}')\n",
    "                    mask_first_char =df_emolex_words['first_char'] == first_char\n",
    "                    df_emolex_words_masked = df_emolex_words.loc[mask_first_char]\n",
    "                    emo_score = df_emolex_words_masked[df_emolex_words_masked.index == word]\n",
    "                    if not emo_score.empty:\n",
    "                        for emotion in list(emotions):\n",
    "                            if emotion != 'first_char':\n",
    "                        # print(emotion)\n",
    "                                emo_df.at[i, emotion] += emo_score[emotion]\n",
    "                            else:\n",
    "                                pass\n",
    "\n",
    "    new_df = pd.concat([new_df, emo_df], axis=1)\n",
    "\n",
    "    return new_df\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(r'take_home_assignment/cleaned_tweets/0_english_lang_cleaned_tweets.csv', sep='\\t')\n",
    "\n",
    "start = datetime.now()\n",
    "print(start)\n",
    "df_emotionally_scored_test = emotional_scoring(df_test, 'cleaned_text', df_emolex_words_outer, emotions)\n",
    "end = datetime.now()\n",
    "print(end)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_emotionally_scored_test.to_csv('scored_tweets_test.csv', sep='\\t', encoding='utf8')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_emotionally_scored_test.head(20)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "globs the cleaned files and runs them through the emotional scoring function and saves them in a separate folder\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cleaned_tweets_path = r'take_home_assignment/cleaned_tweets'\n",
    "def emotional_glob(path):\n",
    "\n",
    "    destination = r'take_home_assignment/emotionally_scored_tweets/'\n",
    "    all_files = glob.glob(path + \"/*.csv\")\n",
    "\n",
    "    counter = 0\n",
    "    for filename in all_files:\n",
    "        df = pd.read_csv(filename, sep = '\\t')\n",
    "        print('start:'+ str(datetime.now()))\n",
    "        df_output = emotional_scoring(df,'cleaned_text')\n",
    "        print('end' + str(datetime.now()))\n",
    "        df_output.to_csv(f'{destination}{counter}_emotional_tweets.csv', sep='\\t'\n",
    "                  , encoding='utf8')\n",
    "        counter+=1\n",
    "\n",
    "\n",
    "\n",
    "emotional_glob(cleaned_tweets_path)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}